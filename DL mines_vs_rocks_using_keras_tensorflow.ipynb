{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "mines-vs-rocks-using-keras-tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_IA4XpPn4xu"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "MsY5lY8Mn4xw"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.pipeline import make_pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "tAOeDokGoC4_",
        "outputId": "1dce1b30-e558-466e-dac8-bdba078e691f"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-09fbca3f-431a-46a9-b654-107a4ca304ca\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-09fbca3f-431a-46a9-b654-107a4ca304ca\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving sonar_data.csv to sonar_data.csv\n",
            "User uploaded file \"sonar_data.csv\" with length 87776 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tqNBh21Gn4xy"
      },
      "source": [
        "SEED = 42\n",
        "from tensorflow.random import set_seed\n",
        "from numpy.random import seed\n",
        "seed(SEED)\n",
        "set_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "collapsed": true,
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": false,
        "id": "t_lpGMD2n4xz"
      },
      "source": [
        "### Loading the dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bs0EHhJGn4x0",
        "outputId": "e15d463b-cbbc-49c7-f950-b0658411090d"
      },
      "source": [
        "df = pd.read_csv('sonar_data.csv', header = None)\n",
        "df = df.values\n",
        "X = df[:,0:60].astype(float)\n",
        "Y = df[:,60]\n",
        "print ('X Shape :', X.shape)\n",
        "print ('Y Shape :', Y.shape)\n",
        "print ('Number of Unique Values in Y:', set(Y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X Shape : (208, 60)\n",
            "Y Shape : (208,)\n",
            "Number of Unique Values in Y: {'R', 'M'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d04fQnWlv004"
      },
      "source": [
        "x=pd.DataFrame(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "-dMrws9jv6RI",
        "outputId": "fd00c608-f3df-44a4-de1a-6f5da1cf5958"
      },
      "source": [
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.0371</td>\n",
              "      <td>0.0428</td>\n",
              "      <td>0.0207</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0986</td>\n",
              "      <td>0.1539</td>\n",
              "      <td>0.1601</td>\n",
              "      <td>0.3109</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>0.1609</td>\n",
              "      <td>0.1582</td>\n",
              "      <td>0.2238</td>\n",
              "      <td>0.0645</td>\n",
              "      <td>0.0660</td>\n",
              "      <td>0.2273</td>\n",
              "      <td>0.3100</td>\n",
              "      <td>0.2999</td>\n",
              "      <td>0.5078</td>\n",
              "      <td>0.4797</td>\n",
              "      <td>0.5783</td>\n",
              "      <td>0.5071</td>\n",
              "      <td>0.4328</td>\n",
              "      <td>0.5550</td>\n",
              "      <td>0.6711</td>\n",
              "      <td>0.6415</td>\n",
              "      <td>0.7104</td>\n",
              "      <td>0.8080</td>\n",
              "      <td>0.6791</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>0.1307</td>\n",
              "      <td>0.2604</td>\n",
              "      <td>0.5121</td>\n",
              "      <td>0.7547</td>\n",
              "      <td>0.8537</td>\n",
              "      <td>0.8507</td>\n",
              "      <td>0.6692</td>\n",
              "      <td>0.6097</td>\n",
              "      <td>0.4943</td>\n",
              "      <td>0.2744</td>\n",
              "      <td>0.0510</td>\n",
              "      <td>0.2834</td>\n",
              "      <td>0.2825</td>\n",
              "      <td>0.4256</td>\n",
              "      <td>0.2641</td>\n",
              "      <td>0.1386</td>\n",
              "      <td>0.1051</td>\n",
              "      <td>0.1343</td>\n",
              "      <td>0.0383</td>\n",
              "      <td>0.0324</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0159</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0167</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.0523</td>\n",
              "      <td>0.0843</td>\n",
              "      <td>0.0689</td>\n",
              "      <td>0.1183</td>\n",
              "      <td>0.2583</td>\n",
              "      <td>0.2156</td>\n",
              "      <td>0.3481</td>\n",
              "      <td>0.3337</td>\n",
              "      <td>0.2872</td>\n",
              "      <td>0.4918</td>\n",
              "      <td>0.6552</td>\n",
              "      <td>0.6919</td>\n",
              "      <td>0.7797</td>\n",
              "      <td>0.7464</td>\n",
              "      <td>0.9444</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8874</td>\n",
              "      <td>0.8024</td>\n",
              "      <td>0.7818</td>\n",
              "      <td>0.5212</td>\n",
              "      <td>0.4052</td>\n",
              "      <td>0.3957</td>\n",
              "      <td>0.3914</td>\n",
              "      <td>0.3250</td>\n",
              "      <td>0.3200</td>\n",
              "      <td>0.3271</td>\n",
              "      <td>0.2767</td>\n",
              "      <td>0.4423</td>\n",
              "      <td>0.2028</td>\n",
              "      <td>0.3788</td>\n",
              "      <td>0.2947</td>\n",
              "      <td>0.1984</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.1306</td>\n",
              "      <td>0.4182</td>\n",
              "      <td>0.3835</td>\n",
              "      <td>0.1057</td>\n",
              "      <td>0.1840</td>\n",
              "      <td>0.1970</td>\n",
              "      <td>0.1674</td>\n",
              "      <td>0.0583</td>\n",
              "      <td>0.1401</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.0621</td>\n",
              "      <td>0.0203</td>\n",
              "      <td>0.0530</td>\n",
              "      <td>0.0742</td>\n",
              "      <td>0.0409</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0125</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0.0191</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>0.1099</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0974</td>\n",
              "      <td>0.2280</td>\n",
              "      <td>0.2431</td>\n",
              "      <td>0.3771</td>\n",
              "      <td>0.5598</td>\n",
              "      <td>0.6194</td>\n",
              "      <td>0.6333</td>\n",
              "      <td>0.7060</td>\n",
              "      <td>0.5544</td>\n",
              "      <td>0.5320</td>\n",
              "      <td>0.6479</td>\n",
              "      <td>0.6931</td>\n",
              "      <td>0.6759</td>\n",
              "      <td>0.7551</td>\n",
              "      <td>0.8929</td>\n",
              "      <td>0.8619</td>\n",
              "      <td>0.7974</td>\n",
              "      <td>0.6737</td>\n",
              "      <td>0.4293</td>\n",
              "      <td>0.3648</td>\n",
              "      <td>0.5331</td>\n",
              "      <td>0.2413</td>\n",
              "      <td>0.5070</td>\n",
              "      <td>0.8533</td>\n",
              "      <td>0.6036</td>\n",
              "      <td>0.8514</td>\n",
              "      <td>0.8512</td>\n",
              "      <td>0.5045</td>\n",
              "      <td>0.1862</td>\n",
              "      <td>0.2709</td>\n",
              "      <td>0.4232</td>\n",
              "      <td>0.3043</td>\n",
              "      <td>0.6116</td>\n",
              "      <td>0.6756</td>\n",
              "      <td>0.5375</td>\n",
              "      <td>0.4719</td>\n",
              "      <td>0.4647</td>\n",
              "      <td>0.2587</td>\n",
              "      <td>0.2129</td>\n",
              "      <td>0.2222</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>0.0176</td>\n",
              "      <td>0.1348</td>\n",
              "      <td>0.0744</td>\n",
              "      <td>0.0130</td>\n",
              "      <td>0.0106</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0164</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0171</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1276</td>\n",
              "      <td>0.0598</td>\n",
              "      <td>0.1264</td>\n",
              "      <td>0.0881</td>\n",
              "      <td>0.1992</td>\n",
              "      <td>0.0184</td>\n",
              "      <td>0.2261</td>\n",
              "      <td>0.1729</td>\n",
              "      <td>0.2131</td>\n",
              "      <td>0.0693</td>\n",
              "      <td>0.2281</td>\n",
              "      <td>0.4060</td>\n",
              "      <td>0.3973</td>\n",
              "      <td>0.2741</td>\n",
              "      <td>0.3690</td>\n",
              "      <td>0.5556</td>\n",
              "      <td>0.4846</td>\n",
              "      <td>0.3140</td>\n",
              "      <td>0.5334</td>\n",
              "      <td>0.5256</td>\n",
              "      <td>0.2520</td>\n",
              "      <td>0.2090</td>\n",
              "      <td>0.3559</td>\n",
              "      <td>0.6260</td>\n",
              "      <td>0.7340</td>\n",
              "      <td>0.6120</td>\n",
              "      <td>0.3497</td>\n",
              "      <td>0.3953</td>\n",
              "      <td>0.3012</td>\n",
              "      <td>0.5408</td>\n",
              "      <td>0.8814</td>\n",
              "      <td>0.9857</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>0.6121</td>\n",
              "      <td>0.5006</td>\n",
              "      <td>0.3210</td>\n",
              "      <td>0.3202</td>\n",
              "      <td>0.4295</td>\n",
              "      <td>0.3654</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>0.1576</td>\n",
              "      <td>0.0681</td>\n",
              "      <td>0.0294</td>\n",
              "      <td>0.0241</td>\n",
              "      <td>0.0121</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0762</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.0394</td>\n",
              "      <td>0.0590</td>\n",
              "      <td>0.0649</td>\n",
              "      <td>0.1209</td>\n",
              "      <td>0.2467</td>\n",
              "      <td>0.3564</td>\n",
              "      <td>0.4459</td>\n",
              "      <td>0.4152</td>\n",
              "      <td>0.3952</td>\n",
              "      <td>0.4256</td>\n",
              "      <td>0.4135</td>\n",
              "      <td>0.4528</td>\n",
              "      <td>0.5326</td>\n",
              "      <td>0.7306</td>\n",
              "      <td>0.6193</td>\n",
              "      <td>0.2032</td>\n",
              "      <td>0.4636</td>\n",
              "      <td>0.4148</td>\n",
              "      <td>0.4292</td>\n",
              "      <td>0.5730</td>\n",
              "      <td>0.5399</td>\n",
              "      <td>0.3161</td>\n",
              "      <td>0.2285</td>\n",
              "      <td>0.6995</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.7262</td>\n",
              "      <td>0.4724</td>\n",
              "      <td>0.5103</td>\n",
              "      <td>0.5459</td>\n",
              "      <td>0.2881</td>\n",
              "      <td>0.0981</td>\n",
              "      <td>0.1951</td>\n",
              "      <td>0.4181</td>\n",
              "      <td>0.4604</td>\n",
              "      <td>0.3217</td>\n",
              "      <td>0.2828</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.1979</td>\n",
              "      <td>0.2444</td>\n",
              "      <td>0.1847</td>\n",
              "      <td>0.0841</td>\n",
              "      <td>0.0692</td>\n",
              "      <td>0.0528</td>\n",
              "      <td>0.0357</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.0046</td>\n",
              "      <td>0.0156</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0107</td>\n",
              "      <td>0.0094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>0.0187</td>\n",
              "      <td>0.0346</td>\n",
              "      <td>0.0168</td>\n",
              "      <td>0.0177</td>\n",
              "      <td>0.0393</td>\n",
              "      <td>0.1630</td>\n",
              "      <td>0.2028</td>\n",
              "      <td>0.1694</td>\n",
              "      <td>0.2328</td>\n",
              "      <td>0.2684</td>\n",
              "      <td>0.3108</td>\n",
              "      <td>0.2933</td>\n",
              "      <td>0.2275</td>\n",
              "      <td>0.0994</td>\n",
              "      <td>0.1801</td>\n",
              "      <td>0.2200</td>\n",
              "      <td>0.2732</td>\n",
              "      <td>0.2862</td>\n",
              "      <td>0.2034</td>\n",
              "      <td>0.1740</td>\n",
              "      <td>0.4130</td>\n",
              "      <td>0.6879</td>\n",
              "      <td>0.8120</td>\n",
              "      <td>0.8453</td>\n",
              "      <td>0.8919</td>\n",
              "      <td>0.9300</td>\n",
              "      <td>0.9987</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8104</td>\n",
              "      <td>0.6199</td>\n",
              "      <td>0.6041</td>\n",
              "      <td>0.5547</td>\n",
              "      <td>0.4160</td>\n",
              "      <td>0.1472</td>\n",
              "      <td>0.0849</td>\n",
              "      <td>0.0608</td>\n",
              "      <td>0.0969</td>\n",
              "      <td>0.1411</td>\n",
              "      <td>0.1676</td>\n",
              "      <td>0.1200</td>\n",
              "      <td>0.1201</td>\n",
              "      <td>0.1036</td>\n",
              "      <td>0.1977</td>\n",
              "      <td>0.1339</td>\n",
              "      <td>0.0902</td>\n",
              "      <td>0.1085</td>\n",
              "      <td>0.1521</td>\n",
              "      <td>0.1363</td>\n",
              "      <td>0.0858</td>\n",
              "      <td>0.0290</td>\n",
              "      <td>0.0203</td>\n",
              "      <td>0.0116</td>\n",
              "      <td>0.0098</td>\n",
              "      <td>0.0199</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0101</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0115</td>\n",
              "      <td>0.0193</td>\n",
              "      <td>0.0157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>0.0323</td>\n",
              "      <td>0.0101</td>\n",
              "      <td>0.0298</td>\n",
              "      <td>0.0564</td>\n",
              "      <td>0.0760</td>\n",
              "      <td>0.0958</td>\n",
              "      <td>0.0990</td>\n",
              "      <td>0.1018</td>\n",
              "      <td>0.1030</td>\n",
              "      <td>0.2154</td>\n",
              "      <td>0.3085</td>\n",
              "      <td>0.3425</td>\n",
              "      <td>0.2990</td>\n",
              "      <td>0.1402</td>\n",
              "      <td>0.1235</td>\n",
              "      <td>0.1534</td>\n",
              "      <td>0.1901</td>\n",
              "      <td>0.2429</td>\n",
              "      <td>0.2120</td>\n",
              "      <td>0.2395</td>\n",
              "      <td>0.3272</td>\n",
              "      <td>0.5949</td>\n",
              "      <td>0.8302</td>\n",
              "      <td>0.9045</td>\n",
              "      <td>0.9888</td>\n",
              "      <td>0.9912</td>\n",
              "      <td>0.9448</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9092</td>\n",
              "      <td>0.7412</td>\n",
              "      <td>0.7691</td>\n",
              "      <td>0.7117</td>\n",
              "      <td>0.5304</td>\n",
              "      <td>0.2131</td>\n",
              "      <td>0.0928</td>\n",
              "      <td>0.1297</td>\n",
              "      <td>0.1159</td>\n",
              "      <td>0.1226</td>\n",
              "      <td>0.1768</td>\n",
              "      <td>0.0345</td>\n",
              "      <td>0.1562</td>\n",
              "      <td>0.0824</td>\n",
              "      <td>0.1149</td>\n",
              "      <td>0.1694</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0080</td>\n",
              "      <td>0.0790</td>\n",
              "      <td>0.1255</td>\n",
              "      <td>0.0647</td>\n",
              "      <td>0.0179</td>\n",
              "      <td>0.0051</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0093</td>\n",
              "      <td>0.0135</td>\n",
              "      <td>0.0063</td>\n",
              "      <td>0.0063</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0062</td>\n",
              "      <td>0.0067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>0.0522</td>\n",
              "      <td>0.0437</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0292</td>\n",
              "      <td>0.0351</td>\n",
              "      <td>0.1171</td>\n",
              "      <td>0.1257</td>\n",
              "      <td>0.1178</td>\n",
              "      <td>0.1258</td>\n",
              "      <td>0.2529</td>\n",
              "      <td>0.2716</td>\n",
              "      <td>0.2374</td>\n",
              "      <td>0.1878</td>\n",
              "      <td>0.0983</td>\n",
              "      <td>0.0683</td>\n",
              "      <td>0.1503</td>\n",
              "      <td>0.1723</td>\n",
              "      <td>0.2339</td>\n",
              "      <td>0.1962</td>\n",
              "      <td>0.1395</td>\n",
              "      <td>0.3164</td>\n",
              "      <td>0.5888</td>\n",
              "      <td>0.7631</td>\n",
              "      <td>0.8473</td>\n",
              "      <td>0.9424</td>\n",
              "      <td>0.9986</td>\n",
              "      <td>0.9699</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8630</td>\n",
              "      <td>0.6979</td>\n",
              "      <td>0.7717</td>\n",
              "      <td>0.7305</td>\n",
              "      <td>0.5197</td>\n",
              "      <td>0.1786</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1446</td>\n",
              "      <td>0.1066</td>\n",
              "      <td>0.1440</td>\n",
              "      <td>0.1929</td>\n",
              "      <td>0.0325</td>\n",
              "      <td>0.1490</td>\n",
              "      <td>0.0328</td>\n",
              "      <td>0.0537</td>\n",
              "      <td>0.1309</td>\n",
              "      <td>0.0910</td>\n",
              "      <td>0.0757</td>\n",
              "      <td>0.1059</td>\n",
              "      <td>0.1005</td>\n",
              "      <td>0.0535</td>\n",
              "      <td>0.0235</td>\n",
              "      <td>0.0155</td>\n",
              "      <td>0.0160</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0051</td>\n",
              "      <td>0.0062</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0138</td>\n",
              "      <td>0.0077</td>\n",
              "      <td>0.0031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>0.0303</td>\n",
              "      <td>0.0353</td>\n",
              "      <td>0.0490</td>\n",
              "      <td>0.0608</td>\n",
              "      <td>0.0167</td>\n",
              "      <td>0.1354</td>\n",
              "      <td>0.1465</td>\n",
              "      <td>0.1123</td>\n",
              "      <td>0.1945</td>\n",
              "      <td>0.2354</td>\n",
              "      <td>0.2898</td>\n",
              "      <td>0.2812</td>\n",
              "      <td>0.1578</td>\n",
              "      <td>0.0273</td>\n",
              "      <td>0.0673</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.2070</td>\n",
              "      <td>0.2645</td>\n",
              "      <td>0.2828</td>\n",
              "      <td>0.4293</td>\n",
              "      <td>0.5685</td>\n",
              "      <td>0.6990</td>\n",
              "      <td>0.7246</td>\n",
              "      <td>0.7622</td>\n",
              "      <td>0.9242</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9979</td>\n",
              "      <td>0.8297</td>\n",
              "      <td>0.7032</td>\n",
              "      <td>0.7141</td>\n",
              "      <td>0.6893</td>\n",
              "      <td>0.4961</td>\n",
              "      <td>0.2584</td>\n",
              "      <td>0.0969</td>\n",
              "      <td>0.0776</td>\n",
              "      <td>0.0364</td>\n",
              "      <td>0.1572</td>\n",
              "      <td>0.1823</td>\n",
              "      <td>0.1349</td>\n",
              "      <td>0.0849</td>\n",
              "      <td>0.0492</td>\n",
              "      <td>0.1367</td>\n",
              "      <td>0.1552</td>\n",
              "      <td>0.1548</td>\n",
              "      <td>0.1319</td>\n",
              "      <td>0.0985</td>\n",
              "      <td>0.1258</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0489</td>\n",
              "      <td>0.0241</td>\n",
              "      <td>0.0042</td>\n",
              "      <td>0.0086</td>\n",
              "      <td>0.0046</td>\n",
              "      <td>0.0126</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0079</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>0.0260</td>\n",
              "      <td>0.0363</td>\n",
              "      <td>0.0136</td>\n",
              "      <td>0.0272</td>\n",
              "      <td>0.0214</td>\n",
              "      <td>0.0338</td>\n",
              "      <td>0.0655</td>\n",
              "      <td>0.1400</td>\n",
              "      <td>0.1843</td>\n",
              "      <td>0.2354</td>\n",
              "      <td>0.2720</td>\n",
              "      <td>0.2442</td>\n",
              "      <td>0.1665</td>\n",
              "      <td>0.0336</td>\n",
              "      <td>0.1302</td>\n",
              "      <td>0.1708</td>\n",
              "      <td>0.2177</td>\n",
              "      <td>0.3175</td>\n",
              "      <td>0.3714</td>\n",
              "      <td>0.4552</td>\n",
              "      <td>0.5700</td>\n",
              "      <td>0.7397</td>\n",
              "      <td>0.8062</td>\n",
              "      <td>0.8837</td>\n",
              "      <td>0.9432</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9375</td>\n",
              "      <td>0.7603</td>\n",
              "      <td>0.7123</td>\n",
              "      <td>0.8358</td>\n",
              "      <td>0.7622</td>\n",
              "      <td>0.4567</td>\n",
              "      <td>0.1715</td>\n",
              "      <td>0.1549</td>\n",
              "      <td>0.1641</td>\n",
              "      <td>0.1869</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>0.1713</td>\n",
              "      <td>0.0959</td>\n",
              "      <td>0.0768</td>\n",
              "      <td>0.0847</td>\n",
              "      <td>0.2076</td>\n",
              "      <td>0.2505</td>\n",
              "      <td>0.1862</td>\n",
              "      <td>0.1439</td>\n",
              "      <td>0.1470</td>\n",
              "      <td>0.0991</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0154</td>\n",
              "      <td>0.0116</td>\n",
              "      <td>0.0181</td>\n",
              "      <td>0.0146</td>\n",
              "      <td>0.0129</td>\n",
              "      <td>0.0047</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0115</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>208 rows Ã— 60 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0       1       2       3   ...      56      57      58      59\n",
              "0    0.0200  0.0371  0.0428  0.0207  ...  0.0180  0.0084  0.0090  0.0032\n",
              "1    0.0453  0.0523  0.0843  0.0689  ...  0.0140  0.0049  0.0052  0.0044\n",
              "2    0.0262  0.0582  0.1099  0.1083  ...  0.0316  0.0164  0.0095  0.0078\n",
              "3    0.0100  0.0171  0.0623  0.0205  ...  0.0050  0.0044  0.0040  0.0117\n",
              "4    0.0762  0.0666  0.0481  0.0394  ...  0.0072  0.0048  0.0107  0.0094\n",
              "..      ...     ...     ...     ...  ...     ...     ...     ...     ...\n",
              "203  0.0187  0.0346  0.0168  0.0177  ...  0.0065  0.0115  0.0193  0.0157\n",
              "204  0.0323  0.0101  0.0298  0.0564  ...  0.0034  0.0032  0.0062  0.0067\n",
              "205  0.0522  0.0437  0.0180  0.0292  ...  0.0140  0.0138  0.0077  0.0031\n",
              "206  0.0303  0.0353  0.0490  0.0608  ...  0.0034  0.0079  0.0036  0.0048\n",
              "207  0.0260  0.0363  0.0136  0.0272  ...  0.0040  0.0036  0.0061  0.0115\n",
              "\n",
              "[208 rows x 60 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnnu08sbn4x0"
      },
      "source": [
        "### Prepping Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKiEDCRpn4x1",
        "outputId": "5f371cd5-3237-48d1-b65f-a42b828c4118"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "Y_encoded = encoder.fit_transform(Y).astype(int)\n",
        "print ('Shape of Y_encoded :', len(Y_encoded))\n",
        "print ('Unique values in Y_encoded :', list(set(Y_encoded)))\n",
        "print ('Inverse transforming : ', encoder.inverse_transform(list(set(Y_encoded))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Y_encoded : 208\n",
            "Unique values in Y_encoded : [0, 1]\n",
            "Inverse transforming :  ['M' 'R']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoDKSmN4n4x2"
      },
      "source": [
        "### Prepping X"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXrBNAWvn4x3",
        "outputId": "81549d45-4376-4e48-97a7-9454326ced74"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        "X_transformed = ss.fit_transform(X)\n",
        "X_transformed.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(208, 60)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UNrp-Gpln4x4"
      },
      "source": [
        "def baseline_model():\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Dense(60, input_dim=(60), activation = 'relu'))\n",
        "    model.add(Dense(1, activation = 'sigmoid'))\n",
        "    \n",
        "    model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'], optimizer = 'adam')\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co5EjiH8n4x4"
      },
      "source": [
        "### Building a single model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6_l3gjYn4x5",
        "outputId": "85cc7855-d782-40dd-ea94-18b1cce97ddf"
      },
      "source": [
        "single_model = baseline_model()\n",
        "%time history = single_model.fit(X_transformed, Y_encoded, epochs = 200, batch_size = 8, verbose = 0, validation_split = 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 19.9 s, sys: 860 ms, total: 20.8 s\n",
            "Wall time: 19.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "o30Te-I2n4x5",
        "outputId": "3066d5f3-e942-41cf-987a-b4420afc4f1a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w5NGTNZn4x5"
      },
      "source": [
        "### Evaluating Model using K-Fold Crossvalidation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jSgYLsE8n4x6"
      },
      "source": [
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "KkJzMANtn4x6"
      },
      "source": [
        "EPOCHS     = 50\n",
        "BATCH_SIZE = 8\n",
        "VERBOSE    = 0\n",
        "FOLDS      = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgwcXq09n4x6",
        "outputId": "43ad0916-58f8-44c9-e8c3-f2e8d49181de"
      },
      "source": [
        "kfold = StratifiedKFold(n_splits = FOLDS, shuffle = True, random_state = SEED)\n",
        "estimators = make_pipeline(StandardScaler(), KerasClassifier(build_fn = baseline_model, epochs = EPOCHS, batch_size = BATCH_SIZE, verbose = VERBOSE))\n",
        "results = cross_val_score(estimators, X, Y_encoded, cv = kfold)\n",
        "print (f'Mean Accuracy : {round(results.mean()*100,2)} %, Std. dev : {round(results.std()*100,2)}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 610 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fd9256d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fdc6880e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fddbf0dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fda2cbd40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fd894d710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fdb4bc0e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fd7087e60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Mean Accuracy : 87.05 %, Std. dev : 7.35%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "xPstbYi0n4x7"
      },
      "source": [
        "### Trying out a small network\n",
        "Reducing the number of hidden layer dimensions after the input dimension to 30 from 60 will put pressure on the network get the most important structure of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1MrKfDun4x7",
        "outputId": "3cd08729-163f-4baa-aaf7-97f9be154c4f"
      },
      "source": [
        "%%time \n",
        "def small_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(30, input_dim=(60), activation = 'relu'))\n",
        "    model.add(Dense(1, activation = 'sigmoid'))\n",
        "    model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'], optimizer = 'adam')\n",
        "    return model\n",
        "\n",
        "kfold = StratifiedKFold(n_splits = FOLDS, shuffle = True, random_state = SEED)\n",
        "estimators = make_pipeline(StandardScaler(), KerasClassifier(build_fn = small_model, epochs = EPOCHS, batch_size = BATCH_SIZE, verbose = VERBOSE))\n",
        "results = cross_val_score(estimators, X, Y_encoded, cv = kfold)\n",
        "print (f'Mean Accuracy : {round(results.mean()*100,2)} %, Std. dev : {round(results.std()*100,2)}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fd5ecec20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fd5e6ab90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fd3bd85f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fddc03f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fdbe704d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fda2cbdd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fddba0680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fe0580830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fd3c53560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fdb501200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Mean Accuracy : 84.14 %, Std. dev : 5.63%\n",
            "CPU times: user 20.4 s, sys: 1.4 s, total: 21.8 s\n",
            "Wall time: 17.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXiw18jrn4x8"
      },
      "source": [
        "We got an equally good model with a smaller network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yJ2W0zAn4x8"
      },
      "source": [
        "### Evaluating a larger network\n",
        "\n",
        "Evaluating a larger Network - A neural network topology with more layers offers more opportunity for the network to extract key features and combined them in useful non-linear ways"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnqhzoCCn4x8",
        "outputId": "e0f92edd-4af7-437a-cbaf-2dbe7394099f"
      },
      "source": [
        "%%time \n",
        "def large_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(60, input_dim=(60), activation = 'relu'))\n",
        "    model.add(Dense(60, activation = 'relu'))\n",
        "    model.add(Dense(1, activation = 'sigmoid'))\n",
        "    model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'], optimizer = 'adam')\n",
        "    return model\n",
        "\n",
        "kfold = StratifiedKFold(n_splits = FOLDS, shuffle = True, random_state = SEED)\n",
        "estimators = make_pipeline(StandardScaler(), KerasClassifier(build_fn = large_model, epochs = EPOCHS, batch_size = BATCH_SIZE, verbose = VERBOSE))\n",
        "results = cross_val_score(estimators, X, Y_encoded, cv = kfold)\n",
        "print (f'Mean Accuracy : {round(results.mean()*100,2)} %, Std. dev : {round(results.std()*100,2)}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fd8971290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fda3a9170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fd29d8290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fd91ad710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fdb501170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fe0580830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fda2cb560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fd5e6a9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fd21727a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f2fd3bf3290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Mean Accuracy : 86.95 %, Std. dev : 7.97%\n",
            "CPU times: user 22.8 s, sys: 1.56 s, total: 24.4 s\n",
            "Wall time: 19.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjuMriUGn4x9"
      },
      "source": [
        "The accuracy improved with increased hidden layer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL7rcYqHogp_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}